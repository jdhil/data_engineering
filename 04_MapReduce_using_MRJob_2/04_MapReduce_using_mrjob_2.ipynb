{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Using `MRJob`: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Posting Dataset\n",
    "\n",
    "The sample dataset we will mainly use (`data/job-data/job-data-2018-09-*.txt`) for this tutorial contains job postings from one of the US job search websites. The data is stored with each row as a JSON document representing a job posting record. \n",
    "\n",
    "The example below shows a sample job postings from the data file. The sample record has been formatted with 4 spaces indentation. In the real file, each record is stored as a JSON document in one row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Example: JSON document of a job posting record*\n",
    "\n",
    "```\n",
    "{\n",
    "    \"industry\": \"Information Technology\", \n",
    "    \"datePosted\": \"2018-09-07\", \n",
    "    \"salaryCurrency\": \"USD\", \n",
    "    \"validThrough\": \"2018-10-07\", \n",
    "    \"empId\": 671932, \n",
    "    \"jobLocation\": {\n",
    "        \"geo\": {\n",
    "            \"latitude\": \"37.7623\", \n",
    "            \"@type\": \"GeoCoordinates\", \n",
    "            \"longitude\": \"-122.4145\"\n",
    "        }, \n",
    "        \"@type\": \"Place\", \n",
    "        \"address\": {\n",
    "            \"postalCode\": \"94110-2042\", \n",
    "            \"addressLocality\": \"San Francisco\", \n",
    "            \"@type\": \"PostalAddress\", \n",
    "            \"addressRegion\": \"CA\", \n",
    "            \"addressCountry\": {\n",
    "                \"@type\": \n",
    "                \"Country\", \n",
    "                \"name\": \"US\"\n",
    "            }\n",
    "        }\n",
    "    }, \n",
    "    \"estimatedSalary\": {\n",
    "        \"@type\": \"MonetaryAmount\", \n",
    "        \"currency\": \"USD\", \n",
    "        \"value\": {\n",
    "            \"maxValue\": \"202000\", \n",
    "            \"@type\": \"QuantitativeValue\", \n",
    "            \"unitText\": \"YEAR\", \n",
    "            \"minValue\": \"146000\"\n",
    "        }\n",
    "    }, \n",
    "    \"description\": \"<div><em>Generate insights and impact from data</em><em>.</em></div>\\n<br/>\\n<div>\\n<div>We're looking for data scientists to join the Analytics team who are excited about applying their analytical skills to understand our users and influence decision making. If you are naturally data curious, excited about deriving insights from data, and motivated by having impact on the business, we want to hear from you.</div><br/>\\n\\n<div><strong>You will:</strong></div><div>\\n\\n\\n<ul>\\n<li>Work closely with product and business teams to identify important questions and answer them with data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Apply statistical and econometric models on large datasets to: i) measure results and outcomes, ii) identify causal impact and attribution, iii) predict future performance of users or products.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Design, analyze, and interpret the results of experiments.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Drive the collection of new data and the refinement of existing data sources.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Create analyses that tell a \\\"story\\\" focused on insights, not just data.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>We're looking for someone with:</strong></div><div>\\n\\n\\n<ul>\\n<li>3+ years experience working with and analyzing large data sets to solve problems.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>A PhD or MS in a quantitative field (e.g., Economics, Statistics, Eng, Natural Sciences, CS).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Expert knowledge of a scientific computing language (such as R or Python) and SQL.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Strong knowledge of statistics and experimental design.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Ability to communicate results clearly and a focus on driving impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>Nice to haves:</strong></div><div>\\n\\n\\n<ul>\\n<li>Prior experience with data-distributed tools (Scalding, Hadoop, Pig, etc).</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div><strong>You should include these in your application:</strong></div><div>\\n\\n\\n<ul>\\n<li>Resume and LinkedIn profile.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Description of the most interesting data analysis you've done, key findings, and its impact.</li>\\n</ul>\\n\\n</div><br/>\\n\\n<div>\\n\\n\\n<ul>\\n<li>Link to or attachment of code you've written related to data analysis.</li>\\n</ul>\\n\\n</div>\\n</div>\\n<br/>\", \n",
    "    \"hiringOrganization\": {\n",
    "        \"@type\": \"Organization\", \n",
    "        \"sameAs\": \"www.stripe.com\", \n",
    "        \"name\": \"Stripe\"\n",
    "    },\n",
    "    \"@type\": \"JobPosting\", \n",
    "    \"jobId\": 2280174543, \n",
    "    \"@context\": \"http://schema.org\", \n",
    "    \"employmentType\": \"FULL_TIME\", \n",
    "    \"occupationalCategory\": [\n",
    "        \"15-1111.00\", \n",
    "        \"Computer and Information Research Scientists\"\n",
    "    ], \n",
    "    \"title\": \"Data Scientist\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy input data to HDFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `job-data': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir job-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `job-data/job-data-2018-09-08.txt': File exists\r\n",
      "put: `job-data/job-data-2018-09-09.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put ../data/job-data/* job-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Top N Pattern\n",
    "\n",
    "\n",
    "Keys:\n",
    "\n",
    "- Top n pattern aims to retrieve a relatively small number of records from a large data set according to a ranking scheme specified by user without sorting the entire data set.\n",
    "- The subset needs to be small enough to fit into one single node and thus N should not be too large.\n",
    "\n",
    "Applications:\n",
    "\n",
    "- Anomaly detection\n",
    "- Finding the top k records with the lowest/highest values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Top N Values\n",
    "\n",
    "In our data set, each job contains two salaires (or no such fields if it's not available), `minValue` and `maxValue`. We want to find out the top N Salaries from all jobs. \n",
    "\n",
    "- To find the top n list across the entire dataset, we need to compare the values from all records, which means the key field becomes less useful and therefore we can assign `None` as the key, which will end up with one single reducer process the reduce job.\n",
    "\n",
    "- To minimize the burden of the reducer and to maximize the parallelism, we can use a combiner to find a local top n list in each mapper container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Find the top N salaries from all jobs. \n",
    "\n",
    "To receive the top N salary list, we first compair `maxValue`, then `minValue`. \n",
    "\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> -> <_, (maxValue, minValue)>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - combiner:`<_, local_top_n[(maxValue, minValue)]>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - reducer:`<_, global_top_n[(maxValue, minValue)]>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`top_n[(maxValue, minValue)]`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  - Ignore key field in `mapper` so only one reducer will be involved  \n",
    "  - `MRJob.reducer_init()` initializes top N sorted list\n",
    "  - `MRJob.reducer()` inserts record into top N sorted list, alway truncate list to a length of N\n",
    "  - `MRJob.reducer_final()` emits record from top N sorted list\n",
    "  - Add `combiner_init`/`combiner`/`combiner_final` steps through `MrJob.steps()` to reduce data flow\n",
    "  - To maintain a sorted list of size n, we use Python built-in heap sort algorithm to achieve O(log(n)) for each insertion operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/3.1_top_n_value.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/3.1_top_n_value.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import heapq\n",
    "\n",
    "\n",
    "class MRTopNValue(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "        \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_passthru_arg('--top_n', type=int)\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            max_ = float(value['estimatedSalary']['value']['maxValue'])\n",
    "            min_ = float(value['estimatedSalary']['value']['minValue'])\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            yield _, (max_, min_)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        if self.options.top_n < 1:\n",
    "            raise ValueError('Invalid top_n value')\n",
    "        self.top_n = []\n",
    "        \n",
    "    def reducer(self, _, values):\n",
    "        for value in values:\n",
    "            if len(self.top_n) < self.options.top_n:\n",
    "                heapq.heappush(self.top_n, value)\n",
    "            else:\n",
    "                heapq.heappushpop(self.top_n, value)\n",
    "                \n",
    "    def reducer_final(self):\n",
    "        for value in self.top_n:\n",
    "            yield None, value\n",
    "            \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,\n",
    "                       combiner_init=self.reducer_init,\n",
    "                       combiner=self.reducer,\n",
    "                       combiner_final=self.reducer_final,\n",
    "                       reducer_init=self.reducer_init,\n",
    "                       reducer=self.reducer,\n",
    "                       reducer_final=self.reducer_final)]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopNValue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/3.hadoop.20180926.223435.193694\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/3.hadoop.20180926.223435.193694...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.1_top_n_value.py ../data/job-data/* --output-dir mr-output --top_n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/3.hadoop.20180926.223438.730336\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180926.223438.730336/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6624719287751284062/] [] /tmp/streamjob7229325374049235081.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0019\n",
      "  Submitted application application_1537993323748_0019\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0019/\n",
      "  Running job: job_1537993323748_0019\n",
      "  Job job_1537993323748_0019 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1537993323748_0019 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=220\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=565\n",
      "\t\tFILE: Number of bytes written=488923\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=220\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=10805248\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4462592\n",
      "\t\tTotal time spent by all map tasks (ms)=10552\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10552\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4358\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4358\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=10552\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4358\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2250\n",
      "\t\tCombine input records=972\n",
      "\t\tCombine output records=20\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=189\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output bytes=24362\n",
      "\t\tMap output materialized bytes=571\n",
      "\t\tMap output records=972\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=672841728\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=571\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40\n",
      "\t\tTotal committed heap usage (bytes)=465043456\n",
      "\t\tVirtual memory (bytes) snapshot=5888532480\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180926.223438.730336...\n",
      "Removing temp directory /tmp/3.hadoop.20180926.223438.730336...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.1_top_n_value.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/ --top_n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Top N Records\n",
    "\n",
    "With the top N Salaries being calculated in file `mr-output/part-00000`, now we want to find out which jobs offer those salaries. \n",
    "\n",
    "Since the top N values are considered small enough to fit into each node, it is preferable to distribute the file into those node where the mapper jobs run, rather than load data in the job configuration. This mechanism is called `Distributed Cache`. We can do it via `MRJob.add_file_arg()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Find all jobs that offer salaries from the top N salaries list. \n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - mapper_init: fetch `top_n_list` to each mapper\n",
    "  - $\\quad\\downarrow$\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record>[if record contains salaries from top_n_list -> <_, record>]`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`record`\n",
    "  \n",
    "- *Features and highlights*:\n",
    "  \n",
    "  - `MRJob.add_file_arg('--cache')` sends an external file to Hadoop\n",
    "  - To send a file to `cache` via command-line: `--cache <file_path>`\n",
    "  - The cached file can then be accessd in script via: `MRJob.options.cache`\n",
    "  - `MRJob.mapper_init()` fetches the top n list from the cached file to each mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/3.2_top_n_job.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/3.2_top_n_job.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "class MRTopNJob(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    OUTPUT_PROTOCOL = JSONValueProtocol\n",
    "        \n",
    "    def configure_args(self):\n",
    "        super().configure_args()\n",
    "        self.add_file_arg('--cache')\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.cache = list()\n",
    "        with open(self.options.cache, 'r') as f:\n",
    "            for line in f:\n",
    "                self.cache.append(tuple(json.loads(line)))\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            max_ = float(value['estimatedSalary']['value']['maxValue'])\n",
    "            min_ = float(value['estimatedSalary']['value']['minValue'])\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            if (max_, min_) in self.cache:\n",
    "                yield _, value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTopNJob.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/3.hadoop.20180926.223523.328890\n",
      "job output is in mr-output-jobs\n",
      "Removing temp directory /tmp/3.hadoop.20180926.223523.328890...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.2_top_n_job.py ../data/job-data/* --output-dir mr-output-jobs --cache mr-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output-jobs\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/3.hadoop.20180926.223527.144662\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180926.223527.144662/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar1665997452787577890/] [] /tmp/streamjob432759349193367946.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0020\n",
      "  Submitted application application_1537993323748_0020\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0020/\n",
      "  Running job: job_1537993323748_0020\n",
      "  Job job_1537993323748_0020 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1537993323748_0020 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output-jobs/\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=49855\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=324020\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=49855\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=10\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=8212480\n",
      "\t\tTotal time spent by all map tasks (ms)=8020\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8020\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8020\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1060\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=122\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=336343040\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=175112192\n",
      "\t\tVirtual memory (bytes) snapshot=3927490560\n",
      "job output is in hdfs:///user/hadoop/mr-output-jobs/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/3.hadoop.20180926.223527.144662...\n",
      "Removing temp directory /tmp/3.hadoop.20180926.223527.144662...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/3.2_top_n_job.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output-jobs/ \\\n",
    "    --cache hdfs:///user/hadoop/mr-output/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inverted Index\n",
    "\n",
    "- Keys:\n",
    "  - Indexing is a technique that is used frequently in almost all seach engine systems. Without an index, the search engine would scan every document in the corpus, which would require considerable time and computing power.\n",
    "  - Inverted index is an index data structure storing a mapping from content, such as keywords, to its locations in a database file, or in a document or a set of documents. \n",
    "  - The purpose of an inverted index is to allow fast full text searches, at a cost of increased processing when a document is added to the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Generate an index from the data set to allow for faster searches on technique skills.\n",
    "\n",
    "The skills we want to search for are listed below:\n",
    "\n",
    "```\n",
    "skillset = ['Java', 'JavaScript', 'C', 'C++', 'C#', 'Python', 'R', 'Bash',\n",
    "            'MySQL', 'Postgresql', 'MongoDB', 'Html', 'Ruby', 'PHP', \n",
    "            'Swift', 'CSS', 'Julia', 'Golang', 'Github', 'Redis', 'Hadoop',\n",
    "            'Spark', 'Hive', 'Pig', 'Spark', 'ElasticSearch', 'Kafka', \n",
    "            'Cassandra', 'AWS', 'GCP', 'Azure', 'Docker', 'kubernetes']\n",
    "```\n",
    "\n",
    "- *Data flow*:\n",
    "\n",
    "  - mapper_init: define search regex pattern\n",
    "  - Input:`record`\n",
    "  - $\\quad\\downarrow$\n",
    "  - mapper:`<_, record> -> find matching skills using regex -> <skill, jobId>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - reducer:`<skill, [jobId]>`\n",
    "  - $\\quad\\downarrow$\n",
    "  - Output:`skill, [jobId]`\n",
    "  \n",
    "Note: Since no aggregation is performed throughout the entire MapReduce pipeline, a combiner in this example is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mr-jobs/4_inverted_index.py\n"
     ]
    }
   ],
   "source": [
    "%%file mr-jobs/4_inverted_index.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.protocol import JSONValueProtocol\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "class MRIndexingSkills(MRJob):\n",
    "    \n",
    "    INPUT_PROTOCOL = JSONValueProtocol\n",
    "    skillset = ['Java', 'JavaScript', 'C', 'C++', 'C#', 'Python', 'R', 'Bash',\n",
    "                'MySQL', 'Postgresql', 'MongoDB', 'Html', 'Ruby', 'PHP', \n",
    "                'Swift', 'CSS', 'Julia', 'Golang', 'Github', 'Redis', 'Hadoop',\n",
    "                'Spark', 'Hive', 'Pig', 'Spark', 'ElasticSearch', 'Kafka', \n",
    "                'Cassandra', 'AWS', 'GCP', 'Azure', 'Docker', 'kubernetes']\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.pattern = re.compile('|'.join(['(?<=\\W){}(?=\\W)'.format(re.escape(x)) for x in self.skillset]), \n",
    "                                  flags=re.IGNORECASE)\n",
    "        \n",
    "    def mapper(self, _, value):\n",
    "        try:\n",
    "            jobId, description = value['jobId'], value['description']\n",
    "        except (KeyError, ValueError):\n",
    "            pass\n",
    "        else:\n",
    "            skills = self.pattern.findall(description)\n",
    "            for skill in skills:\n",
    "                yield skill.capitalize(), jobId\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        yield key, list(values)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRIndexingSkills.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/4_inverted_index.hadoop.20180926.223603.127426\n",
      "job output is in mr-output\n",
      "Removing temp directory /tmp/4_inverted_index.hadoop.20180926.223603.127426...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/4_inverted_index.py ../data/job-data/* --output-dir mr-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run on your Hadoop cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs:///user/hadoop/mr-output\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hdfs:///user/hadoop/mr-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /usr/local/hadoop-2.8.4/bin...\n",
      "Found hadoop binary: /usr/local/hadoop-2.8.4/bin/hadoop\n",
      "Using Hadoop version 2.8.4\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop-2.8.4...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop-2.8.4/share/hadoop/tools/lib/hadoop-streaming-2.8.4.jar\n",
      "Creating temp directory /tmp/4_inverted_index.hadoop.20180926.223614.867498\n",
      "Copying local files to hdfs:///user/hadoop/tmp/mrjob/4_inverted_index.hadoop.20180926.223614.867498/files/...\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8642215883446702203/] [] /tmp/streamjob8243278373205048200.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input files to process : 2\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1537993323748_0021\n",
      "  Submitted application application_1537993323748_0021\n",
      "  The url to track the job: http://c8d937eb6693:8088/proxy/application_1537993323748_0021/\n",
      "  Running job: job_1537993323748_0021\n",
      "  Job job_1537993323748_0021 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1537993323748_0021 completed successfully\n",
      "  Output directory: hdfs:///user/hadoop/mr-output/\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8849806\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=60350\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=100659\n",
      "\t\tFILE: Number of bytes written=688034\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8850042\n",
      "\t\tHDFS: Number of bytes written=60350\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=20150272\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4801536\n",
      "\t\tTotal time spent by all map tasks (ms)=19678\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=19678\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4689\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4689\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=19678\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=4689\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2520\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=168\n",
      "\t\tInput split bytes=236\n",
      "\t\tMap input records=1772\n",
      "\t\tMap output bytes=90635\n",
      "\t\tMap output materialized bytes=100665\n",
      "\t\tMap output records=5009\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=655134720\n",
      "\t\tReduce input groups=30\n",
      "\t\tReduce input records=5009\n",
      "\t\tReduce output records=30\n",
      "\t\tReduce shuffle bytes=100665\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=10018\n",
      "\t\tTotal committed heap usage (bytes)=450363392\n",
      "\t\tVirtual memory (bytes) snapshot=5886922752\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/hadoop/mr-output/\n",
      "Removing HDFS temp directory hdfs:///user/hadoop/tmp/mrjob/4_inverted_index.hadoop.20180926.223614.867498...\n",
      "Removing temp directory /tmp/4_inverted_index.hadoop.20180926.223614.867498...\n"
     ]
    }
   ],
   "source": [
    "!python3 mr-jobs/4_inverted_index.py \\\n",
    "-r hadoop hdfs:///user/hadoop/job-data/ \\\n",
    "    --output-dir mr-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
